# vibe-local .env.example
# Copy this file to .env and edit as needed.
# .env is loaded from the current working directory and overrides ~/.config/vibe-local/config.
#
# Priority (last wins): ~/.config/vibe-local/config < .env < environment variables < CLI flags

# --- API Endpoint ---
# Default: http://localhost:11434 (Ollama)
# Examples:
#   Ollama (default):        OLLAMA_HOST=http://localhost:11434
#   llama.cpp server:        OLLAMA_HOST=http://localhost:8080
#   LM Studio:               OLLAMA_HOST=http://localhost:1234
#   vLLM (local):            OLLAMA_HOST=http://localhost:8000
#   OpenAI:                  OLLAMA_HOST=https://api.openai.com/v1
#   Groq:                    OLLAMA_HOST=https://api.groq.com/openai/v1
#   Together AI:             OLLAMA_HOST=https://api.together.xyz/v1
OLLAMA_HOST=http://localhost:11434

# --- Model ---
# Leave empty to auto-detect (Ollama only).
# For non-Ollama backends, specify the model name explicitly.
# Examples:
#   MODEL=qwen3:8b
#   MODEL=llama3.1:8b
#   MODEL=gpt-4o
#   MODEL=llama-3.1-8b-instant
MODEL=

# --- Sidecar Model (optional) ---
# Lightweight model used for context compaction and permission checks.
# Auto-selected when using Ollama. Leave empty for non-Ollama backends.
SIDECAR_MODEL=

# --- API Key ---
# Required for cloud APIs (OpenAI, Groq, Together AI, etc.).
# Leave empty for local backends (Ollama, llama.cpp, LM Studio).
API_KEY=

# --- Debug ---
# Set to 1 to enable verbose debug logging.
# VIBE_LOCAL_DEBUG=0
